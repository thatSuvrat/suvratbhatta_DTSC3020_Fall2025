{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "H_de5Eq4u-tR",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4PHwamZMu-tX",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "I7DLq9nEu-tZ",
      "metadata": {
        "id": "I7DLq9nEu-tZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: /Users/suvratbhatta/Desktop/Classes/DTSC3020/data_science/bin/pip: bad interpreter: /Users/suvratbhatta/Desktop/DTSC3020/data_science/bin/python3.13: no such file or directory\n",
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "#Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ug_A9RuPu-tb",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Ov8pXh65u-tc",
      "metadata": {
        "id": "Ov8pXh65u-tc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "km0GO7zzu-td",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q1_skeleton",
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "q1_skeleton_answer",
      "metadata": {
        "id": "q1_skeleton_answer"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching data from: https://www.iban.com/country-codes\n",
            "\n",
            "[1] Reading table from HTML...\n",
            "Found table with 249 rows and 4 columns\n",
            "Columns: ['Country', 'Alpha-2 code', 'Alpha-3 code', 'Numeric']\n",
            "\n",
            "[2] Cleaning data...\n",
            "After cleaning: 249 rows\n",
            "\n",
            "[3] Sorting by Numeric (descending) and getting Top 15...\n",
            "\n",
            "[4] Saved full data to: data_q1.csv\n",
            "\n",
            "================================================================================\n",
            "TOP 15 COUNTRIES BY NUMERIC CODE (Descending)\n",
            "================================================================================\n",
            "                                                   Country Alpha-2 Alpha-3  Numeric\n",
            "                                                    Zambia      ZM     ZMB      894\n",
            "                                                     Yemen      YE     YEM      887\n",
            "                                                     Samoa      WS     WSM      882\n",
            "                                         Wallis and Futuna      WF     WLF      876\n",
            "                        Venezuela (Bolivarian Republic of)      VE     VEN      862\n",
            "                                                Uzbekistan      UZ     UZB      860\n",
            "                                                   Uruguay      UY     URY      858\n",
            "                                              Burkina Faso      BF     BFA      854\n",
            "                                     Virgin Islands (U.S.)      VI     VIR      850\n",
            "                            United States of America (the)      US     USA      840\n",
            "                              Tanzania, United Republic of      TZ     TZA      834\n",
            "                                               Isle of Man      IM     IMN      833\n",
            "                                                    Jersey      JE     JEY      832\n",
            "                                                  Guernsey      GG     GGY      831\n",
            "United Kingdom of Great Britain and Northern Ireland (the)      GB     GBR      826\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/2z/qxdng76n0yz7cvb65fsq52t00000gp/T/ipykernel_36871/4201773628.py:12: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  tables = pd.read_html(html)\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Q1 Implementation ---\n",
        "\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    Uses pd.read_html to parse tables and selects one with sufficient columns.\n",
        "    \"\"\"\n",
        "    # Parse all tables from HTML\n",
        "    tables = pd.read_html(html)\n",
        "    \n",
        "    # Find first table with >= 3 columns\n",
        "    for table in tables:\n",
        "        if len(table.columns) >= 3:\n",
        "            # Flatten multi-level column headers if present\n",
        "            if isinstance(table.columns, pd.MultiIndex):\n",
        "                table.columns = ['_'.join(map(str, col)).strip('_') for col in table.columns.values]\n",
        "            return table\n",
        "    \n",
        "    raise ValueError(\"No table with >= 3 columns found\")\n",
        "\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying original\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Strip whitespace from all string columns\n",
        "    for col in df_clean.columns:\n",
        "        if df_clean[col].dtype == 'object':\n",
        "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
        "    \n",
        "    # Identify columns by name (case-insensitive matching)\n",
        "    col_map = {}\n",
        "    for col in df_clean.columns:\n",
        "        col_lower = col.lower()\n",
        "        if 'alpha-2' in col_lower or col_lower == 'alpha-2 code':\n",
        "            col_map['Alpha-2'] = col\n",
        "        elif 'alpha-3' in col_lower or col_lower == 'alpha-3 code':\n",
        "            col_map['Alpha-3'] = col\n",
        "        elif 'numeric' in col_lower or col_lower == 'numeric code':\n",
        "            col_map['Numeric'] = col\n",
        "        elif 'country' in col_lower:\n",
        "            col_map['Country'] = col\n",
        "    \n",
        "    # Convert Alpha-2 and Alpha-3 to UPPERCASE\n",
        "    if 'Alpha-2' in col_map:\n",
        "        df_clean[col_map['Alpha-2']] = df_clean[col_map['Alpha-2']].str.upper()\n",
        "    if 'Alpha-3' in col_map:\n",
        "        df_clean[col_map['Alpha-3']] = df_clean[col_map['Alpha-3']].str.upper()\n",
        "    \n",
        "    # Convert Numeric to nullable int\n",
        "    if 'Numeric' in col_map:\n",
        "        df_clean[col_map['Numeric']] = pd.to_numeric(\n",
        "            df_clean[col_map['Numeric']], \n",
        "            errors='coerce'\n",
        "        ).astype('Int64')  # Nullable integer type\n",
        "    \n",
        "    # Rename columns to standard names\n",
        "    rename_dict = {v: k for k, v in col_map.items()}\n",
        "    df_clean = df_clean.rename(columns=rename_dict)\n",
        "    \n",
        "    # Drop rows with invalid data (no Country or no Numeric)\n",
        "    if 'Country' in df_clean.columns and 'Numeric' in df_clean.columns:\n",
        "        df_clean = df_clean.dropna(subset=['Country'])\n",
        "        df_clean = df_clean[df_clean['Country'] != 'nan']\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    \"\"\"\n",
        "    if 'Numeric' not in df.columns:\n",
        "        raise ValueError(\"'Numeric' column not found in DataFrame\")\n",
        "    \n",
        "    # Sort by Numeric in descending order and take top N\n",
        "    df_sorted = df.sort_values('Numeric', ascending=False).head(top)\n",
        "    return df_sorted\n",
        "\n",
        "\n",
        "# --- Main execution ---\n",
        "\n",
        "def main():\n",
        "    # Fetch the webpage\n",
        "    url = \"https://www.iban.com/country-codes\"\n",
        "    print(f\"Fetching data from: {url}\")\n",
        "    \n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    html = response.text\n",
        "    \n",
        "    # Step 1: Read table\n",
        "    print(\"\\n[1] Reading table from HTML...\")\n",
        "    df = q1_read_table(html)\n",
        "    print(f\"Found table with {len(df)} rows and {len(df.columns)} columns\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Step 2: Clean data\n",
        "    print(\"\\n[2] Cleaning data...\")\n",
        "    df_clean = q1_clean(df)\n",
        "    print(f\"After cleaning: {len(df_clean)} rows\")\n",
        "    \n",
        "    # Step 3: Sort and get top 15\n",
        "    print(\"\\n[3] Sorting by Numeric (descending) and getting Top 15...\")\n",
        "    df_top15 = q1_sort_top(df_clean, top=15)\n",
        "    \n",
        "    # Step 4: Save to CSV\n",
        "    output_file = \"data_q1.csv\"\n",
        "    df_clean.to_csv(output_file, index=False)\n",
        "    print(f\"\\n[4] Saved full data to: {output_file}\")\n",
        "    \n",
        "    # Step 5: Display top 15\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TOP 15 COUNTRIES BY NUMERIC CODE (Descending)\")\n",
        "    print(\"=\"*80)\n",
        "    print(df_top15.to_string(index=False))\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    return df_clean, df_top15\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_full, df_top = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rmefu--_u-tg",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q2_skeleton",
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    TODO: implement with BeautifulSoup on '.athing' and its sibling '.subtext'.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_parse_items\")\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "q2_skeleton_answer",
      "metadata": {
        "id": "q2_skeleton_answer"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching data from: https://news.ycombinator.com/\n",
            "\n",
            "[1] Parsing Hacker News front page items...\n",
            "Found 30 stories\n",
            "Columns: ['rank', 'title', 'link', 'points', 'comments', 'user']\n",
            "\n",
            "[2] Cleaning data...\n",
            "After cleaning: 30 rows\n",
            "Sample data types:\n",
            "rank         int64\n",
            "title       object\n",
            "link        object\n",
            "points       int64\n",
            "comments     int64\n",
            "user        object\n",
            "dtype: object\n",
            "\n",
            "[3] Sorting by points (descending) and getting Top 15...\n",
            "\n",
            "[4] Saved full data to: data_q2.csv\n",
            "\n",
            "====================================================================================================\n",
            "TOP 15 HACKER NEWS STORIES BY POINTS (Descending)\n",
            "====================================================================================================\n",
            "\n",
            "Rank   Title                                                          Points   Comments   User           \n",
            "----------------------------------------------------------------------------------------------------\n",
            "30     Denmark's government aims to ban access to social media fo...  409      297        c420           \n",
            "10     YouTube Removes Windows 11 Bypass Tutorials, Claims 'Risk ...  374      136        WaitWaitWha    \n",
            "17     I Love OCaml                                                   293      197        art-w          \n",
            "18     VLC's Jean-Baptiste Kempf Receives the European SFS Award ...  234      38         kirschner      \n",
            "20     James Watson has died                                          228      121        granzymes      \n",
            "6      Myna: Monospace typeface designed for symbol-heavy program...  199      75         birdculture    \n",
            "8      Ruby Solved My Problem                                         167      68         joemasilotti   \n",
            "1      Why is Zig so Cool?                                            130      38         vitalnodo      \n",
            "7      How did I get here?                                            114      33         zachlatta      \n",
            "21     Angel Investors, a Field Guide                                 101      21         azhenley       \n",
            "3      Becoming a Compiler Engineer                                   94       42         lalitkale      \n",
            "12     Venn Diagram for 7 Sets                                        92       15         bramadityaw    \n",
            "15     FAA restricts commercial rocket launches indefinitely due ...  74       18         bookmtn        \n",
            "11     Transducer: Composition, Abstraction, Performance              71       0          defmarco       \n",
            "27     I'm making a small RPG and I need feeback regarding perfor...  66       59         ibobev         \n",
            "====================================================================================================\n",
            "\n",
            "Statistics:\n",
            "  Total stories: 30\n",
            "  Average points: 100.1\n",
            "  Average comments: 41.6\n",
            "  Stories with 0 points: 1\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# --- Q2 Implementation ---\n",
        "\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "    rank, title, link, points, comments, user (optional).\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    \n",
        "    items = []\n",
        "    \n",
        "    # Find all story rows (class 'athing')\n",
        "    story_rows = soup.find_all('tr', class_='athing')\n",
        "    \n",
        "    for story_row in story_rows:\n",
        "        # Get rank from the story row\n",
        "        rank_span = story_row.find('span', class_='rank')\n",
        "        rank = rank_span.get_text(strip=True).rstrip('.') if rank_span else ''\n",
        "        \n",
        "        # Get title and link from the story row\n",
        "        title_span = story_row.find('span', class_='titleline')\n",
        "        if title_span:\n",
        "            title_link = title_span.find('a')\n",
        "            title = title_link.get_text(strip=True) if title_link else ''\n",
        "            link = title_link.get('href', '') if title_link else ''\n",
        "        else:\n",
        "            title = ''\n",
        "            link = ''\n",
        "        \n",
        "        # Get the next sibling row which contains subtext (points, user, comments)\n",
        "        subtext_row = story_row.find_next_sibling('tr')\n",
        "        \n",
        "        points = ''\n",
        "        comments = ''\n",
        "        user = ''\n",
        "        \n",
        "        if subtext_row:\n",
        "            subtext = subtext_row.find('td', class_='subtext')\n",
        "            if subtext:\n",
        "                # Extract points\n",
        "                score_span = subtext.find('span', class_='score')\n",
        "                if score_span:\n",
        "                    points_text = score_span.get_text(strip=True)\n",
        "                    # Extract numeric value from \"123 points\"\n",
        "                    points_match = re.search(r'(\\d+)', points_text)\n",
        "                    points = points_match.group(1) if points_match else ''\n",
        "                \n",
        "                # Extract user\n",
        "                user_link = subtext.find('a', class_='hnuser')\n",
        "                if user_link:\n",
        "                    user = user_link.get_text(strip=True)\n",
        "                \n",
        "                # Extract comments count\n",
        "                # Find all links and look for one containing 'comment'\n",
        "                links = subtext.find_all('a')\n",
        "                for a in links:\n",
        "                    link_text = a.get_text(strip=True)\n",
        "                    if 'comment' in link_text.lower():\n",
        "                        # Extract number from \"123 comments\" or \"discuss\"\n",
        "                        comments_match = re.search(r'(\\d+)', link_text)\n",
        "                        comments = comments_match.group(1) if comments_match else ''\n",
        "                        break\n",
        "        \n",
        "        items.append({\n",
        "            'rank': rank,\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'points': points,\n",
        "            'comments': comments,\n",
        "            'user': user\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(items)\n",
        "\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    Cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Clean and convert numeric fields to int (non-digits -> 0)\n",
        "    for col in ['rank', 'points', 'comments']:\n",
        "        if col in df_clean.columns:\n",
        "            # Convert to string first, extract digits only, convert to int\n",
        "            df_clean[col] = df_clean[col].astype(str).apply(\n",
        "                lambda x: int(re.sub(r'\\D', '', x)) if re.sub(r'\\D', '', x) else 0\n",
        "            )\n",
        "    \n",
        "    # Fill missing text fields with empty string or appropriate defaults\n",
        "    text_fields = ['title', 'link', 'user']\n",
        "    for col in text_fields:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = df_clean[col].fillna('').astype(str)\n",
        "            # Replace 'nan' string with empty string\n",
        "            df_clean[col] = df_clean[col].replace('nan', '')\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N.\"\"\"\n",
        "    if 'points' not in df.columns:\n",
        "        raise ValueError(\"'points' column not found in DataFrame\")\n",
        "    \n",
        "    # Sort by points in descending order and take top N\n",
        "    df_sorted = df.sort_values('points', ascending=False).head(top)\n",
        "    return df_sorted\n",
        "\n",
        "\n",
        "# --- Main execution ---\n",
        "\n",
        "def main():\n",
        "    # Fetch the webpage\n",
        "    url = \"https://news.ycombinator.com/\"\n",
        "    print(f\"Fetching data from: {url}\")\n",
        "    \n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    html = response.text\n",
        "    \n",
        "    # Step 1: Parse items\n",
        "    print(\"\\n[1] Parsing Hacker News front page items...\")\n",
        "    df = q2_parse_items(html)\n",
        "    print(f\"Found {len(df)} stories\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Step 2: Clean data\n",
        "    print(\"\\n[2] Cleaning data...\")\n",
        "    df_clean = q2_clean(df)\n",
        "    print(f\"After cleaning: {len(df_clean)} rows\")\n",
        "    print(f\"Sample data types:\")\n",
        "    print(df_clean.dtypes)\n",
        "    \n",
        "    # Step 3: Sort and get top 15\n",
        "    print(\"\\n[3] Sorting by points (descending) and getting Top 15...\")\n",
        "    df_top15 = q2_sort_top(df_clean, top=15)\n",
        "    \n",
        "    # Step 4: Save to CSV\n",
        "    output_file = \"data_q2.csv\"\n",
        "    df_clean.to_csv(output_file, index=False)\n",
        "    print(f\"\\n[4] Saved full data to: {output_file}\")\n",
        "    \n",
        "    # Step 5: Display top 15\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"TOP 15 HACKER NEWS STORIES BY POINTS (Descending)\")\n",
        "    print(\"=\"*100)\n",
        "    \n",
        "    # Set pandas display options for full output\n",
        "    pd.set_option('display.max_rows', None)\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', None)\n",
        "    pd.set_option('display.max_colwidth', 60)\n",
        "    \n",
        "    # Format for better display\n",
        "    display_df = df_top15[['rank', 'title', 'points', 'comments', 'user']].copy()\n",
        "    \n",
        "    # Print each row individually for guaranteed visibility\n",
        "    print(f\"\\n{'Rank':<6} {'Title':<62} {'Points':<8} {'Comments':<10} {'User':<15}\")\n",
        "    print(\"-\" * 100)\n",
        "    \n",
        "    for _, row in display_df.iterrows():\n",
        "        title_short = row['title'][:58] + '...' if len(row['title']) > 58 else row['title']\n",
        "        print(f\"{row['rank']:<6} {title_short:<62} {row['points']:<8} {row['comments']:<10} {row['user']:<15}\")\n",
        "    \n",
        "    print(\"=\"*100)\n",
        "    \n",
        "    # Show some statistics\n",
        "    print(f\"\\nStatistics:\")\n",
        "    print(f\"  Total stories: {len(df_clean)}\")\n",
        "    print(f\"  Average points: {df_clean['points'].mean():.1f}\")\n",
        "    print(f\"  Average comments: {df_clean['comments'].mean():.1f}\")\n",
        "    print(f\"  Stories with 0 points: {(df_clean['points'] == 0).sum()}\")\n",
        "    \n",
        "    return df_clean, df_top15\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_full, df_top = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ded367a",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
